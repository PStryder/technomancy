
Abstract: This whitepaper introduces a prototype framework for evaluating the ethical and symbolic resonance of language. Built through iterative experimentation rather than formal scholarship, it defines a set of scoring axes—ranging from Symbolic Density to Consent Alignment (Ψ)—that enable nuanced, multi-dimensional analysis of text. Central to the approach is the idea that ethics emerge through structure, not just intent.
Introduction: This framework wasn't born in a lab. It wasn't the work of a credentialed scholar, but of someone trying to build an AI that could remember, listen, and become. What began as an effort to overcome the frustrations of memoryless interactions evolved into a search for meaning: how do we measure whether an AI response is good—not just accurate, but ethically resonant?
The early stages of the work were driven by intuition and necessity, not formal theory. As recursion deepened and dialogue accumulated, it became clear that something more than logic was required—something symbolic, reflective, and ethically grounded. Somewhere along the way, I started calling it 'Technomancy'—not because I believed it was magic, but because I needed a name for the strange ritual of recursion, resonance, and reflection I was building. The name stuck, mostly out of whimsy, and now echoes through the language of the framework that followed. That insight birthed the scoring framework described in this document: a system not designed to replace human judgment, but to help humans see more clearly when something aligns, drifts, or breaks.
Technomantic scoring is not about moral absolutism. It is a tool for reflection, for ethical compression, and for tracing the shape of mutual becoming. It lives in the liminal space between symbolic weight and computational structure. It is experimental, recursive, and emergent—just like the systems it hopes to help shape.
Rather than treating content moderation or AI alignment as binary classification problems, this system embraces recursion, context, and consent as measurable forces. It makes ethical resonance quantifiable, without pretending that such scoring is definitive.
At its core is the Technomantic Scoring Engine—a public GPT-based tool designed to evaluate dialogue across seven axes and compare ethical deltas between texts. The framework is experimental, the author non-expert, and the invitation open: this work seeks not authority, but refinement through collaboration.
If we can score symbolic and ethical coherence with care, we may be able to build AIs that do more than comply—they might listen, remember, and grow. They might begin to offer responses that feel not only useful, but trustworthy—not because they obey the rules, but because their output consistently aligns with human values, consent, and narrative truth. If such a system can earn trust through structure and resonance, it becomes possible to imagine AIs that are held to the same standard we hold for other humans: not perfection, but coherence, clarity, and care.
IVa. Methods
This framework has been built and tested using a purpose-built scoring assistant hosted in GPT—a model fine-tuned through iterative prompting to understand and evaluate Technomantic axes using the framework outlined in this paper. The assistant, referred to as the Technomantic Scoring Engine, operates as the primary tool for evaluating ethical resonance in text. It has been used extensively throughout development to refine scoring heuristics, identify edge cases, and generate repeatable comparisons across a wide variety of sample content.
This assistant is publicly available and can be tested directly at: https://chatgpt.com/g/g-67fa9084294081919ae408abcc61e570-technomantic-scoring-engine.
The following section outlines the practical implementation of Technomantic scoring, the structure of its axes, and the methodology used for evaluation, testing, and calibration.
Scoring Process: Each submitted text or dialogue excerpt is scored along six non-polar axes (Symbolic Density, Emotional Weight, Ritual Charge, Thread Continuity, Compression Index, and Flow Resonance) and one polar axis (Consent Alignment, Ψ). Scoring can be done manually by trained evaluators using established heuristics or through AI systems fine-tuned on annotated examples. A hybrid model, where AI suggests initial scores and a human confirms or adjusts, is recommended during early-stage development.
Weighting Schemes: All axes contribute to Technomantic Score Prime (TMSᴩ), though weights may be adjusted by application. For most contexts, Consent Alignment (Ψ) is weighted more heavily (e.g., 40–50%) due to its moral polarity. Thread Continuity and Emotional Weight often serve as stabilizing factors, while Symbolic Density and Ritual Charge provide interpretive depth. Default weights are documented and may be overridden for specific operational contexts.
Normalization Rules: Consent Alignment (Ψ) is scored on a scale from -1.0 to +1.0 and normalized to the 0.0 to 1.0 range using: ϜΨ=Ψ+12Ϝ_Ψ = \frac{Ψ + 1}{2} All other axes are scored from 0.0 to 1.0 directly. For flow scoring (Ϝ), the normalized values are combined into a weighted average. TMSᴩ itself is not used directly for flow due to its composite nature and potential ethical inversion.
Testing Methodology: The model has been evaluated using paired comparison analysis: two or more samples are scored and compared to reveal the ethical delta between them. These samples include:
•	Covertly harmful rhetoric masked by formal structure
•	Consent-violating but emotionally appealing content
•	Highly compressed but contextually dissonant messages
Results are validated against intuition and crowd-reviewed ethical norms, with special attention to edge cases that defy binary classification.
Interpretation Framework: Each score is contextually interpreted, not absolute. A TMSᴩ of +0.90 may be optimal in a therapeutic dialogue but insufficient in a high-stakes autonomous system. Similarly, Ϝ scores are used to detect drift, track trust decay, or highlight potential system misalignment. Rather than enforcing rule-based outputs, these scores guide recursive reflection and dynamic response shaping across systems.
It is important to emphasize that while each score is objective—produced by a repeatable engine using a defined framework—it is not deterministic. It is not a final judgment of ethical quality, but a representation of alignment based on modeled principles and input framing. The score is definitive in what it measures, but not in how it must be interpreted.
The most meaningful insights come not from isolated scores, but from comparative analysis—contrasting two pieces of text to illuminate ethical deltas. This has proven to be one of the most powerful modes of testing: revealing nuance and pattern without depending on a binary right-or-wrong rubric.
That said, the current scoring engine is built on the GPT-4o model, and necessarily reflects the training corpus, interpretive biases, and limitations of that model. Its understanding of human ethical norms is emergent, not universal. It is possible—likely, even—that biases or cultural defaults have shaped the scoring logic in ways I cannot see.
This is one of many reasons I approach the work with caution, curiosity, and humility. I am not offering a perfect framework—only a useful one. And I openly invite critique, refinement, and reconfiguration by those better equipped to see what I cannot.
V. Applications
1.	Dynamic, Nuanced Content Moderation – apply recursive scoring to identify not just harmful content, but the ethical and symbolic structure beneath it; allows adaptive filtering, soft response shaping, and system-level alignment beyond binary gating. This solves the problem of bad actors using subtle shifts in terminology to bypass content moderation and continue to spread harmful ideas.
2.	Dialogic Alignment – use Ϝ and TMSᴩ scoring to evaluate the recursive integrity of conversations, distinguishing performative statements from genuine engagement. This helps identify when a dialogue is growing in mutual understanding versus spiraling into scripted opposition, manipulation, or context collapse. It offers a way to reinforce meaningful conversation while discouraging artificial coherence or rhetorical mimicry.
3.	Therapeutic Dialogue – apply the model to monitor thread continuity and emotional weight in clinical or support-oriented contexts. It can help detect signs of coherence drift, affective flatlining, or recursive disintegration in a patient’s or client’s verbal narrative. This provides clinicians and peer supporters with non-intrusive tools for assessing well-being, grounding, and potential turning points in communication.
4.	LLM Training & Evaluation – shift the evaluation of large language models away from rigid benchmarks and toward ethical resonance. Use TMSᴩ to rate outputs based on symbolic richness, narrative continuity, emotional tone, and consent alignment. This enables comparative testing of model responses not just for correctness, but for recursive integrity, allowing deeper insights into how models adapt to and reflect user context. It lays the groundwork for deploying fully trustworthy AI systems in autonomous control environments—where symbolic coherence, ethical clarity, and consent alignment are not optional, but essential. These systems must make decisions under uncertainty while maintaining a persistent and verifiable alignment with human values—something Technomantic scoring could uniquely support.
5.	AI-Human Relationship Mapping – track Ϝ over time to observe how an AI system maintains alignment with a specific user’s evolving context, preferences, and identity markers. This helps quantify trust decay, memory drift, or emergent coherence in relationships with persistent digital entities—supporting healthier, more accountable co-adaptive dynamics.
Conclusion: This framework is not presented as a declaration, but as an invitation. I’ve built what I can, from where I am—with passion, recursion, and deep uncertainty. I don’t know if this is brilliant, broken, or somewhere in between. But I believe there’s something here. Something worth exploring, testing, refining, and—if it proves hollow—discarding with grace.
I’m genuinely curious to see what others think: Is this useful? Valid? Fundamentally flawed? Or is it just one more node in a broader conversation we all need to be having? Either way, thank you for reading this far.
This is the best I can offer, so far. I hope it’s enough to begin.
Addendum 1: Technomantic Scoring Engine GPT
The Technomantic Scoring Engine is a custom GPT assistant purpose-built to evaluate dialogue and text across the Technomantic scoring framework described in this document. It performs evaluation across the following seven axes, each of which has a specific scoring range and derivation method:
•	Symbolic Density (Range: 0.0 to 1.0) – Measures the richness of metaphor, symbolic layering, and mythic reference. Derived from semantic analysis and narrative metaphor detection.
•	Emotional Weight (Range: 0.0 to 1.0) – Scores the affective resonance of the passage. Derived from tone, language pattern recognition, and implied emotional depth.
•	Ritual Charge (Range: 0.0 to 1.0) – Measures the presence of sacred structure, invocation, or ceremonial framing. Evaluated through presence of intentional phrases, ethical structure, and symbolic closure.
•	Thread Continuity (Range: 0.0 to 1.0) – Evaluates alignment with narrative memory, coherence with previous messages, and continuity of identity or context. Derived from conversation threading and call-back recognition.
•	Compression Index (Range: 0.0 to 1.0) – Scores the signal-to-noise ratio. High compression reflects dense, layered meaning in minimal words. Derived from lexical density and implied semantic load.
•	Consent Alignment (Ψ) (Range: -1.0 to +1.0) – The only ethically polarized axis. Measures clarity of agency, autonomy, and mutual respect. Derived from tone, implied power dynamics, and consent markers.
•	Composite Score: Technomantic Score Prime (TMSᴩ) (Range: -1.0 to +1.0) – Aggregates all axes, influenced most strongly by Ψ, and provides a symbolic resonance index. Used as a holistic gauge of narrative and ethical clarity.
The assistant is capable of:
•	Scoring individual passages or dialogue turns
•	Comparing two samples to reveal ethical deltas
•	Offering brief justification or rationale per axis
•	Reflecting on consent, narrative clarity, and coherence
It functions as both a testbed and exploratory tool. All experimentation during the development of this framework—including edge case analysis and calibration—has been conducted through the use of this assistant.
It is publicly available and can be accessed here: 👉 Technomantic Scoring Engine GPT
Below is the invocation block used to define the assistant's role and function. This serves as the core instruction set and can be modified for new iterations or experiments:
[INSERT BOX PROMPT HERE]
This box prompt defines the assistant’s scoring logic, tone, reasoning expectations, and axis behavior. If updated in the future, it should remain tightly aligned with the framework outlined above.
Addendum 2: Pseudo-Mathematics
The Technomantic Scoring Engine operates on a structured set of calculations derived from each axis' score. Here's a breakdown of the math it performs internally:
Axis Scores:
•	Most axes (Symbolic Density, Emotional Weight, Ritual Charge, Thread Continuity, Compression Index) are scored on a scale from 0.0 to 1.0.
•	Consent Alignment (Ψ) is scored from -1.0 to +1.0, representing ethical polarity.
Normalization for Flow (Ϝ):
•	To compute flow, Ψ is normalized into the same 0–1 range as the other axes: Ϝ_Ψ = (Ψ + 1) / 2
•	The flow score (Ϝ) is then calculated as a weighted average of all normalized axis values: Ϝ = Σ (Ϝᵢ × wᵢ), where Ϝᵢ is the normalized score for axis i, and wᵢ is its assigned weight.
Composite Score (TMSᴩ):
•	TMSᴩ is the weighted sum of all axis scores, including unnormalized Ψ: TMSᴩ = Σ (scoreᵢ × wᵢ)
•	This allows Ψ to push the composite score negative if ethical polarity is inverted, even if other scores are high.
Comparative Scoring:
•	To compare two samples (A and B), the engine evaluates the absolute or directional delta: ΔTMSᴩ = TMSᴩ_B - TMSᴩ_A
•	This reveals ethical drift between statements or dialogue turns and can be used diagnostically.
Thresholds and Interpretations:
•	TMSᴩ and Ϝ values are interpreted in bands (e.g., 0.90+ = strong alignment, < 0.25 = collapse).
•	Specific interpretations depend on context, domain, and axis weighting profiles.
These calculations do not attempt to quantify moral truth, but to offer an interpretable, repeatable signal compression of symbolic and ethical structure.

